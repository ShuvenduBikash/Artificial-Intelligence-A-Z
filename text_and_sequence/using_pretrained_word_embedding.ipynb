{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING PRETRAINED WORD EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "imdb_dir = 'E:\\\\Datasets\\\\aclImdb'\n",
    "train_dir = os.path.join(imdb_dir, 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load tha dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            try:\n",
    "                f = open(os.path.join(dir_name, fname))\n",
    "                texts.append(f.read())\n",
    "                f.close()\n",
    "\n",
    "                if label_type == 'neg':\n",
    "                    labels.append(0)\n",
    "                else:\n",
    "                    labels.append(1)\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24984\n",
      "24984\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\n"
     ]
    }
   ],
   "source": [
    "print(labels[0])\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tekenize the raw IMDB text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 100\n",
    "training_samples = 200\n",
    "validation_samples = 10000\n",
    "max_words = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62, 4, 3, 129, 34, 45, 7648, 1412, 15, 3, 4267, 514, 43, 16, 3, 633, 133, 12, 6, 3, 1302, 460, 4, 1749, 209, 3, 7744, 308, 6, 677, 80, 32, 2138, 1111, 3016, 31, 1, 929, 4, 42, 5123, 469, 9, 2685, 1749, 1, 223, 55, 16, 54, 828, 1325, 856, 228, 9, 40, 96, 122, 1485, 56, 145, 36, 1, 997, 141, 26, 677, 122, 1, 411, 59, 94, 2282, 303, 773, 5, 3, 835, 20, 3, 1757, 647, 42, 125, 71, 22, 235, 101, 16, 46, 49, 624, 31, 710, 84, 710, 379, 3504, 2, 8430, 67, 26, 107, 3371]\n"
     ]
    }
   ],
   "source": [
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print(word_index['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88413 unique tokens\n"
     ]
    }
   ],
   "source": [
    "print('Found %s unique tokens'%len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  34   45 7648 1412   15    3 4267  514   43   16    3  633  133   12    6\n",
      "    3 1302  460    4 1749  209    3 7744  308    6  677   80   32 2138 1111\n",
      " 3016   31    1  929    4   42 5123  469    9 2685 1749    1  223   55   16\n",
      "   54  828 1325  856  228    9   40   96  122 1485   56  145   36    1  997\n",
      "  141   26  677  122    1  411   59   94 2282  303  773    5    3  835   20\n",
      "    3 1757  647   42  125   71   22  235  101   16   46   49  624   31  710\n",
      "   84  710  379 3504    2 8430   67   26  107 3371]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(data[0])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24984, 100)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24984,)\n"
     ]
    }
   ],
   "source": [
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12433  3548  4928 ..., 11712  8415 18871]\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train and test, before that shuffles the data\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[indices]\n",
    "labels = labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 100)\n",
      "(10000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing the GloVe Word-embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors\n"
     ]
    }
   ],
   "source": [
    "glove_dir = 'E:\\\\Datasets\\\\glove.6B'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding=\"utf8\")\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.27085999  0.044006   -0.02026    -0.17395     0.6444      0.71213001\n",
      "  0.35510001  0.47138    -0.29637     0.54426998 -0.72294003 -0.0047612\n",
      "  0.040611    0.043236    0.29729     0.10725     0.40156001 -0.53662002\n",
      "  0.033382    0.067396    0.64556003 -0.085523    0.14103     0.094539\n",
      "  0.74947    -0.19400001 -0.68739003 -0.41740999 -0.22807001  0.12       -0.48999\n",
      "  0.80944997  0.045138   -0.11898     0.20161     0.39276001 -0.20121001\n",
      "  0.31354001  0.75304002  0.25907001 -0.11566    -0.029319    0.93498999\n",
      " -0.36067     0.52420002  0.23706     0.52714998  0.22869    -0.51958001\n",
      " -0.79348999 -0.20367999 -0.50186998  0.18748     0.94282001 -0.44834\n",
      " -3.67919993  0.044183   -0.26751     2.19970012  0.241      -0.033425\n",
      "  0.69553    -0.64472002 -0.0072277   0.89574999  0.20015     0.46493\n",
      "  0.61932999 -0.1066      0.08691    -0.4623      0.18262    -0.15849\n",
      "  0.020791    0.19373     0.063426   -0.31672999 -0.48177001 -1.38479996\n",
      "  0.13669001  0.96859002  0.049965   -0.27379999 -0.035686   -1.05770004\n",
      " -0.24467     0.90366    -0.12442     0.080776   -0.83401     0.57200998\n",
      "  0.088945   -0.42532    -0.018253   -0.079995   -0.28580999 -0.01089\n",
      " -0.4923      0.63687003  0.23642001]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_index['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100)\n"
     ]
    }
   ],
   "source": [
    "# Preparing GloVe word embeding matrix\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        # Get the vector from original GloVe data\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        \n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                320032    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,320,065\n",
      "Trainable params: 1,320,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the pretrained word embeddings into the Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
