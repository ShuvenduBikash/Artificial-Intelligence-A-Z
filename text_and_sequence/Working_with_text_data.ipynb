{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with text data\n",
    "Like all other neural networks, deep-learning models don’t take as input raw text:\n",
    "they only work with numeric tensors. Vectorizing text is the process of transforming text\n",
    "into numeric tensors. This can be done in multiple ways:\n",
    "1. Segment text into words, and transform each word into a vector.\n",
    "2. Segment text into characters, and transform each character into a vector.\n",
    "3. Extract n-grams of words or characters, and transform each n-gram into a vector.\n",
    "N-grams are overlapping groups of multiple consecutive words or characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding n-grams and bag-of-words\n",
    "Word n-grams are groups of N (or fewer) consecutive words that you can extract from\n",
    "a sentence. The same concept may also be applied to characters instead of words.\n",
    "Here’s a simple example. Consider the sentence “The cat sat on the mat.” It may be\n",
    "decomposed into the following set of 2-grams: <br/>\n",
    "{\"The\", \"The cat\", \"cat\", \"cat sat\", \"sat\",\n",
    "\"sat on\", \"on\", \"on the\", \"the\", \"the mat\", \"mat\"}<br/><br/>\n",
    "It may also be decomposed into the following set of 3-grams:<br/>\n",
    "{\"The\", \"The cat\", \"cat\", \"cat sat\", \"The cat sat\",\n",
    "\"sat\", \"sat on\", \"on\", \"cat sat on\", \"on the\", \"the\",\n",
    "\"sat on the\", \"the mat\", \"mat\", \"on the mat\"}<br/><br/>\n",
    "Such a set is called a bag-of-2-grams or bag-of-3-grams, respectively. The term bag\n",
    "here refers to the fact that you’re dealing with a set of tokens rather than a list or\n",
    "sequence: the tokens have no specific order. This family of tokenization methods is\n",
    "called bag-of-words.<br/>\n",
    "Because bag-of-words isn’t an order-preserving tokenization method (the tokens generated are understood as a set, not a sequence, and the general structure of the sentences is lost), it tends to be used in shallow language-processing models rather than\n",
    "in deep-learning models. Extracting n-grams is a form of feature engineering, and\n",
    "deep learning does away with this kind of rigid, brittle approach, replacing it with hierarchical feature learning. One-dimensional convnets and recurrent neural networks,\n",
    "introduced later in this chapter, are capable of learning representations for groups of\n",
    "words and characters without being explicitly told about the existence of such groups,\n",
    "by looking at continuous word or character sequences. For this reason, we won’t\n",
    "cover n-grams any further in this book. But do keep in mind that they’re a powerful,\n",
    "unavoidable feature-engineering tool when using lightweight, shallow text-processing\n",
    "models such as logistic regression and random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-level one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ate': 8, 'my': 9, 'mat.': 6, 'the': 5, 'The': 1, 'cat': 2, 'dog': 7, 'homework.': 10, 'sat': 3, 'on': 4}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "token_index = {}\n",
    "for sample in samples:\n",
    "    for word in sample.split():\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index) + 1\n",
    "            \n",
    "print(token_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]]\n"
     ]
    }
   ],
   "source": [
    "max_length = 10\n",
    "\n",
    "results = np.zeros(shape=(len(samples), max_length, max(token_index.values()) + 1))\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = token_index.get(word)\n",
    "        results[i, j, index] = 1.\n",
    "        \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using keras for word-label one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 4, 7, 8, 1, 9], [1, 6, 5, 2, 3]]\n"
     ]
    }
   ],
   "source": [
    "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
    "\n",
    "# config to take 1000 words\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "\n",
    "# Build the index for words\n",
    "tokenizer.fit_on_texts(samples)\n",
    "\n",
    "# Turns the string into lists of integer indices\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sat': 7, 'homework': 3, 'cat': 4, 'ate': 5, 'the': 1, 'dog': 6, 'my': 2, 'on': 8, 'mat': 9}\n",
      "[[ 0.  1.  0. ...,  0.  0.  0.]\n",
      " [ 0.  1.  1. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(word_index)\n",
    "print(one_hot_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
